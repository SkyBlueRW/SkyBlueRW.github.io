#

## The Practical Side of Black-Litterman and More


- [Introduction](#introduction)
- [The Practical Side of Black-Litterman](#practical)
  - [The Black Litterman Tau](#subparagraph1)
  - [The Subjective Omega](#subparagraph2)
- [More on Expected Return Forecast with Black-Litterman](#more)
  - [The Baseline From Benchmark](#subparagraph3)
  - [The Refinement of Multiple Forecasts](#subparagraph4)
- [Reference](#ref)

### Introduction <a name="introduction"></a>

Forecasting expected return is a rewarding yet challenging endeavor. One shot to tilt the odds in our favor a bit in this battle is probably to take into account as much relavant information as possible. In such cases, a reliable method to weave together these information of distinct categories and forms becomes quite crutial. 

The Black-Litterman model stands as a dependable solution in this quest. As we explored in the previous blog ([Anchor Your Forecast the Bayesian Way](https://skybluerw.github.io/2023/07/27/anchor-forecast-bayesian.html)), the model serves a dual role. On one hand, it safeguards our baseline by anchoring the refined estimates to a benchmark portfolio. On the other, it opens the door to upward potentialy by allowing us to seamlessly integrate a multitude of forecats from various models, all within a risk-aware framework.

This model serves as an excellent starting point in our expedition to refine expected return forecasts. While there's still a vast landscape to explore ahead! Within this blog, let's embark on a deeper dive into the practical side of the Black-Litterman model and close with a causual chat on the two perspectives of the framework just mentioned.

### The Practical Side of Black-Litterman <a name="practical"></a>

![BL](https://raw.githubusercontent.com/SkyBlueRW/SkyBlueRW.github.io/main/_posts/asset/bl.png)

Let's pick up where we left off in our exploration of the Black-Litterman model in the previous blog ([Anchor Your Forecast the Bayesian Way](https://skybluerw.github.io/2023/07/27/anchor-forecast-bayesian.html)). Without too many assumptions placed so far, we get a refined forecast that combines all original forecasts and prior in the form of posterior.

$$
\begin{aligned}
\mu^{\star} &= (\Sigma_{\pi} + P^T\Omega^{-1}P)^{-1}(\Sigma_{\pi}^{-1} \pi + P^T\Omega^{-1}Q) \\
M &= (\Sigma_{\pi} + P^T\Omega^{-1}P)^{-1}
\end{aligned}
$$

This is a general framework, with which, we are free to feed in all kinds of forecasts (prior and likelihood) within the Gaussian world. However, applying such a general model demands significant estimation in application. Consider, for instance, the two matrices that quantify uncertainty ($$\Sigma_{\pi}$$ and $$\Omega$$) alone, we have $$N^2 + K^2$$ parameters to estimate. (N and K refers to number of securities in the universe and number of original forecast respectively). 

To streamline the estimation process and mitigate model risk originated from large number of estimations, a lot of techniques with further restriction upon the Gassuan assumption are applied in application. Let's look into some of these techniques that faciliate the practical application of the framework.


#### The Black Litterman Tau <a name="subparagraph1"></a>

One authentic assumption made by Black & Litterman themselves is that the covariance matrix measuring uncertainty around prior ($$\Sigma_{\pi}$$) is proportional to return covariance matrix ($$\Sigma_{\pi} = \tau \Sigma$$). Remeber that, to maintain a solid baseline, the prior mean is extracted from reverse optimization of benchmark portfolio wihout clearly defined uncertainty. This one additional assumption instantly free us from the estimation of $$N^2$$ somewhat vague parameters.

Certainly, it is not the 'correct and accurate' assumption, while it does capture some key points from the full picture and significantly eases the tension on estimation. In practice, higher volatility in security returns generally translates to greater uncertainty in estimating expected returns. 

With the tau factor in play, we arrive at the standard Black-Litterman estimatior in the following form:

$$
\begin{aligned}
\mu^{\star} &= (\tau \Sigma + P^T\Omega^{-1}P)^{-1}((\tau \Sigma)^{-1} \pi + P^T\Omega^{-1}Q) \\
M &= (\tau \Sigma + P^T\Omega^{-1}P)^{-1}
\end{aligned}
$$

Now, the scaling factor $$\tau$$ is all required to determine the unceratainty around prior. Typically, $$\tau$$ is assigned heuristically with a small value close to 0 as "uncertainty in the mean is much smaller than the uncertainty in the return itself" (Black & Litterman (1992)). 

Broadly speaking, this argument aligns with other methods proposed to set $$\tau$$. Whether to take the uncertainty as sampling error ($$\tau = \dfrac{1}{T}$$) or to infer it from holding in risky asset in reference portfolio ($$\tau = \dfrac{1}{w_{risky}} - 1$$). These approachs generally lead to a $$\tau$$ at the range within 0 and 0.1. [Walters (2013)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1701467) provides a very comprehensive review about the parameter $$\tau$$.


#### The Subjective Omega <a name="subparagraph2"></a>

With $$\tau$$ neatly defining uncertainty around the prior, let's take a more detailed look into the uncertainty around likelihood. The goal is to embrace a wide array kinds of investment views in the form of likelihood.

It is relatively straightforward to set the uncertainty around investment views generated from statistical models. Forecasting errors, such as the residual variance in a regression, naturally find their place in this context. However, similar to prior mean extracted from reverse optimization, it is indirect to deal with investment views that don't emerge from quantitative methods and lack forecast errors (i.e., relying on subjective judgment). Incorporating such investment views demands extra effort and consideration.

Drawing inspiration from Black and Litterman's approach to model prior uncertainty, we can extend a similar principle to other investment views â€” linking their uncertainty to return variance. After all, it's the relative level of uncertainty that truly matters. For a subjective investment view k expressed as a linear portfolio ($$p_k$$), we can assume that the uncertainty is scaled based on the portfolio variance. 

Now the question turn to the setting of $$\alpha$$.

$$
\begin{aligned}
\omega_k &= \alpha P_k \Sigma P_k^T
\end{aligned}
$$

It's worth noting that when we apply this structure of $$\tau$$ and $$\alpha$$ to define the uncertainty around the prior and likelihood,  the ratio of $$\dfrac{\tau}{\alpha}$$ alone is sufficient to define our posterior mean ($$\mu^{\star}$$). It is the existence of posterior variance (M) that necessitates the seprate estimation of $$\tau$$ and $$\alpha$$.

$$
\begin{aligned}
\mu^{\star} &= (\tau \Sigma + P^T\Omega^{-1}P)^{-1}((\tau \Sigma)^{-1} \pi + P^T\Omega^{-1}Q) \\
 &= \pi + \tau \Sigma P^T(\alpha P\Sigma P^T + \tau P\Sigma P^T)^{-1} (Q-P\pi) \\
&= \pi + \Sigma P^T (\dfrac{\alpha}{\tau} P\Sigma P^T +  P\Sigma P^T)^{-1} (Q-P\pi)\\
M &= (\tau \Sigma + P^T\Omega^{-1}P)^{-1} \\ 
& = (1 + \tau)\Sigma - \tau^2\Sigma P^T (\tau P \Sigma P^T + \Omega)^{-1}P \Sigma
\end{aligned}
$$

In practice, our primary focus often lies with the posterior mean rather than the posterior variance. This is probably the reason why such a simplification of $$\dfrac{\tau}{\alpha}$$ is untilized in a lot of applications. It should be noted that, though retains many of the metris of the full scale framework, such approach is no longer a full-fledged Bayesian framework. 

There are also paths to uphold the sanctity of a full-fledged Bayesian framework with both posterior mean and variance provided.

**Idozek Implied Confidence Level**

Following the definition of $$\omega$$ in $$\alpha$$, Idozek(2007) introduced a remarkably intuitive method to infer $$\alpha$$ from the percentage of confidence, as defined by the desired level of active deviation. This approach paints a clear picture of the translation from a subjective confidence level to $$\omega$$.

Depending on our confidence in a particular investment view k, our refined posterior mean range from $$\mu^{\star}_{0}$$ (in case of 0 confidence in the view/ $$\alpha \to \infty$$) to $$\mu^{\star}_{100}$$ (in case of absolute confidence in the view $$\alpha = 0$$). Correspondingly, our final portfolio built on the refined estimation would range from the most conservative portfolio $$x^{\star}_0$$  to the most confident portfolio $$x^{\star}_{100}$$.

$$
\begin{aligned}
\mu^{\star} &= \pi + \Sigma_{\pi}P_k^T(\Omega + P_k\Sigma_{\pi}P_k^T)^{-1} (Q-P_k\pi) \\
\mu^{\star}_{100} &= \pi + \tau\Sigma P_k^T (P_k\tau \Sigma P_k^T)^{-1}(Q - P_k\pi)\\
\mu^{\star}_0 &= \pi \\
x^{\star}_{100} &= (\lambda \Sigma)^{-1} \mu^{\star}_{100}\\
x^{\star}_0 &= (\lambda \Sigma)^{-1} \pi = x_0\\
\end{aligned}
$$

A confidence level (0% ~ 100%) can be defined in terms of the desired position ($$\hat{x}$$) between the two portfolios of $$x^{\star}_0$$ and $$x^{\star}_{100}$$.    

$$
\begin{aligned}
confidence &= \dfrac{\hat{x} - x^{\star}_0}{x^{\star}_{100} - x^{\star}_0} \\
\hat{x} &= x^{\star}_0 + confidence * (x^{\star}_{100} - x^{\star}_0) \\
\end{aligned}
$$

This approach offers an intuitive and consistent means of expressing confidence. On one hand, it provides the flexibility to express confidence in percentage from 0% to 100%. On the other hand, this percentage of confidence is **linearly proportional to our active weight and risk**. As we move our confidence from 0% to 100%, the active weight progresses linearly from $$x^{\star}_0$$ to $$x^{\star}_{100}$$ and so does the active risk.

With the method in place, we simplify the estimation of $$\omega$$ by linking it with a well defined percentange of confidence level. All we need is to back out the $$\omega$$ that leads to this portfolio with desired deviation ($$\hat{x}$$). Walters (2007) provides an analytical solution of this portfolio leading even more simplied results. 

$$
\begin{aligned}
\dfrac{1}{1 + \alpha} &= \dfrac{\hat{x} - x^{\star}_0}{x^{\star}_{100} - x^{\star}_0} = confidence \\
&\downarrow \\
\alpha &= \dfrac{1 - confidence}{confidence} \\ 
\omega_k & = \alpha p_k \Sigma p_k^T \\
\end{aligned}
$$

This approach can be applied systematically to all investment views lacking clearly defined uncertainty, effectively expanding the utility of the Black-Litterman framework to accommodate a wide array of investment perspectives, as long as subjective judgment on confidence level is available.

**Haesen, Hallerbach, Markwat & Molenaar tau/alpha ratio**

There are also ways to undertake the $$\dfrac{\tau}{\alpha}$$ within a full fledged Bayesian framework. For application in asset allocation, Haesen, Hallerbach, Markwat & Molenaar (2017) introduced another modification that simplifies the Black-Litterman framework. They suggest assuming the availability of only absolute views on all securities and directly defining the ratio of $$\dfrac{\tau}{\alpha} $$instead of treating them as separate parameters. This strategic restriction transforms the matrix P into an identity matrix, resulting in significant simplification.

$$
\begin{aligned}
P &= I_N \\
&\downarrow \\
\mu^{\star} &= (1 + \dfrac{\tau}{\alpha})^{-1} (\pi + \dfrac{\tau}{\alpha}Q) \\
M &= (\tau^{-1} + \alpha^{-1}) \Sigma
\end{aligned}
$$

Now within the mix of investment views, what truly matters is the ratio $$\dfrac{\tau}{\alpha}$$. The weights assigned to these views in the refined forecast are **linearly proportional to the ratio**.

$$
\begin{aligned}
\alpha \to 0 (\dfrac{\tau}{\alpha} \to \infty)\\
\mu^{\star} &= Q\\
M &\to 0\\
\alpha = \tau  (\dfrac{\tau}{\alpha} = \dfrac{1}{2})\\
\mu^{\star} &= \dfrac{1}{2} (Q + \pi) \\
M &= \dfrac{1}{2} \tau \Sigma \\
\alpha \to \infty (\dfrac{\tau}{\alpha} \to 0)\\
\mu^{\star} &= \pi \\
M &= \tau \Sigma \\
\end{aligned}
$$


### More on Expected Return Forecast with Black-Litterman <a name="more"></a>

The techniques we've explored thus far should prepare us reasonably to put the Black-Litterman model in application. Before closing the mark here, I want to have some causual chat on the ideas and thoughts within all discussed so far.

To me, the Bayesian framework that we discussed all along, at its core, is the powerful tool to combine different forecasts in a risk-aware way. Take the refinement of expected return as the construction of a building as analogy, the additional structures placed aim to start the construction from a solid basement (prior from benchmark) and efficiently build more and more levels on top of it (a risk-aware combination of different forecasts).

#### The Baseline From Benchmark <a name="subparagraph3"></a>

The prior, whether extracted from a reverse optimization acts the basement in our quest. On one hand, it becomes the 'numeraire' in the forecasts combination. Every other forecasts have been compared to the prior (also with each other) for the judgement of reliability. On the other hand, in case of only partial coverage from other forecasts (that is other forecasts do not span all securities), it would be used to complete a full-fledged forecast

Black & Litterman chose to use the expected return reversely optimized from market portfolio as this foundation. Such a prior is not only consistent with the market portfolio but also well supported by the CAPM model. 

At the cost of a theoretical foundation like CAPM model, we are free to choose any benchmark portfolio for the prior extraction via reverse optimization. It make a lot of sense in a financial market where most portfolios are benchmarked to some reference portfolio. After all, it's the comparison against the benchmark portfolio that determins the performance and more. Haesen, Hallerbach, Markwat & Molenaar (2017) proposed to use risk parity portfolio for extraction of prior for the application of asset allocation.

I would also like to highlight that, reverse optimization is definitely not the only way for prior. We can choose other forecasts as prior. There are actually not that much difference between prior and likelihood in the framework. They are all forecasts on expected return. 

In my opnion, the desirable traits that makes one forecast stand out as the prior is two folds. Firstly, the prior should cover on every security we are interesed in within the universe, hence a well defined estimation on all securities are valaiable even in case of missing coverage from other forecasts. Additionally, it would be best if it is a robust estimation that is not sensitive to noise or vary dramatically hence such desirable characteristics can be carried over the refined estimation as well.



Under such premise, other forecasts such as one from multi-factor models .. should also fit the shoes.

#### The Refinement of Multiple Forecasts <a name="subparagraph4"></a>

with basement set, feed improve. worth discuss validity of multiple. effeorts deserve.

To me, in the quest for refined expected return forecasts, the idea of combining multiple forecasts presents a tantalizing prospect, rich with potential benefits from various perspectives.

**The Finance Perspective**

The world of financial markets is a labyrinth of complexity, where a myriad of factors exert their influence on expected returns across different time horizons and securities. Yet, due to the inherent challenges of modeling and data limitations, there's no single, unified model or framework capable of a comprehensive review on these factors all at once. Instead, the approach to expected return forecasting often follows the wisdom of "divide and conquer."

Consider, for instance, the long-term horizon spanning decades. Here, fundamental variables like population growth and technological advancements play a pivotal role in forecasting returns on asset classes. Zoom in a bit, and we're focusing on a horizon of a few years, where variables related to the business cycle, such as consumption and production, take center stage in forecasting returns on asset classes, sectors, and more. As you narrow down to shorter horizons, company-specific characteristics become the focal point for equity forecasting.

Likewise, the impact of various factors varies significantly across distinct types of securities. Take, for instance, the realm of equities, where company-specific attributes like value and quality assume paramount significance in the pricing equation. In contrast, within the domain of debt, a different narrative unfolds, with factors such as solvency and leverage assuming a position of preeminence. Meanwhile, in the universe of commodities, it is the ebb and flow of inflation rates that wield substantial influence.

With these specialized models, each calibrated for specific horizons and selected subsets of securities, the rationale for combining forecasts becomes much clearer. By aggregating insights generated from these diverse models, we piece together a more comprehensive view of the vast and intricate financial landscape, enabling us to navigate its complexities with greater clarity and insight.

**The Data Science Perspective**

The notion of combining multiple forecasts intersects with a trailblazing effort in data science aimed at enhancing forecast accuracy. Envision it as akin to the concept of ensemble learning, which ranks among the top strategies in many machine learning competitions. The fundamental premise of ensemble learning is to amalgamate individual models to craft a more robust meta-model.

Drawing a parallel to the Black-Litterman model, think of it as an ensemble model comprising a pool of distinct individual models. Within this ensemble, those models exhibiting relatively greater strength are accorded higher weights. This mirrors the essence of the Black-Litterman approach, where investment views endowed with lower uncertainty wield greater influence over the final refined estimate.

In essence, it's reminiscent of a fusion of bagging and stacking techniques. Heterogeneous models are applied to diverse datasets, and this methodology thrives when applied to a varied array of modelsâ€”an apt analogy for the diverse landscape of expected return forecasting.



#### More on Black-Litterman <a name="subparagraph4"></a>


Now turn back to Black-Litterman model and the associated Bayesian framework again. At its core, it takes various expected return forecast likely generated from different models and combine them into one refined new estimation. In this process, although named and used differently, the difference between the forecast in terms of prior and the forecast in terms of likelihood is smaller than it seems. They are all taken as forecast on expected returns with uncertainties. 

In my view, what make the pior stand out from all expected forecast are two folds. For one hand, the prior forecast do have opnion on every security within the universe, which build the base line for other partial forecasts. 

a lot of factor impacting expected return. looking into different horizon asset type lead to different key drivers. hard to get an integrated model. divide and conquer is a solution. 




### Reference <a name="ref"></a>

- Walters (2013): The Factor Tau in the Black-Litterman Model
- Walters (2007): The Black-Litterman Model in Details
- Idzorek (2007): A step-by-step guide to the Black-Litterman model
- Haesen, Hallerbach, Markwat & Molenaar (2017): Enhancing Risk Parity By Including Views
