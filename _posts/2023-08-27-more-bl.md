#

## The Practical Side of Black-Litterman and More


- [Introduction](#introduction)
- [The Practical Side of Black-Litterman](#practical)
  - [The Black Litterman Tau](#subparagraph1)
  - [The Subjective Omega](#subparagraph2)
- [More on Expected Return Forecast](#more)
  - [The Refinement of Multiple Forecasts](#subparagraph3)
  - [More on Black-Litterman](#subparagraph4)
- [Reference](#ref)

### Introduction <a name="introduction"></a>

Forecasting expected return is a rewarding yet challenging endeavor. One shot to tilt the odds in our favor a bit in this battle is probably to take into account as much relavant information as possible. In such cases, a reliable method to weave together these information of distinct categories and forms becomes quite crutial. 

The Black-Litterman model stands as a dependable solution in this quest. As we explored in the previous blog ([Anchor Your Forecast the Bayesian Way](https://skybluerw.github.io/2023/07/27/anchor-forecast-bayesian.html)), this model serves a dual purpose. It guard our baseline by anchoring the refined estimates to a benchmark portfolio. Simultaneously, it unlocks the potential for upper side by enabling the incorporation of diverse forecasts from various models, all within a risk-aware framework.

The Black-Litterman model serves as an excellent starting point in our expedition to refine expected return forecasts. While there's still a vast landscape to explore ahead!! Within this blog, let's embark on a deeper dive into the practical side of the Black-Litterman model. We will wrap up with a causual discussion on the idea of incorporating multiple forecasts and its connection with the Black-Litterman model.

### The Practical Side of Black-Litterman <a name="practical"></a>

![BL](https://raw.githubusercontent.com/SkyBlueRW/SkyBlueRW.github.io/main/_posts/asset/bl.png)

Let's pick up where we left off in our exploration of the Black-Litterman model in the previous blog ([Anchor Your Forecast the Bayesian Way](https://skybluerw.github.io/2023/07/27/anchor-forecast-bayesian.html)). Without too many assumptions placed so far, we get a refined forecast that combines all original forecast/information in the form of posterior.

$$
\begin{aligned}
\mu^{\star} &= (\Sigma_{\pi} + P^T\Omega^{-1}P)^{-1}(\Sigma_{\pi}^{-1} \pi + P^T\Omega^{-1}Q) \\
M &= (\Sigma_{\pi} + P^T\Omega^{-1}P)^{-1}
\end{aligned}
$$

This is a general framework, with which, we are free to feed in all kinds of forecasts (prior and likelihood) within the Gaussian world. However, applying such a general model demands significant estimation in application. Consider, for instance, the two matrices that quantify uncertainty ($$\Sigma_{\pi}$$ and $$\Omega$$) alone, we have $$N^2 + K^2$$ parameters to estimate. (N and K refers to number of securities in the universe and number of original forecast respectively). 

To streamline the estimation process and mitigate model risk originated from large number of estimations, a lot of techniques with further restriction upon the Gassuan assumption are applied in application. Let's look into some of these techniques that faciliate the practical application of the framework.


#### The Black Litterman Tau <a name="subparagraph1"></a>

One authentic assumption made by Black & Litterman themselves is that the covariance matrix measuring uncertainty around prior ($$\Sigma_{\pi}$$) is proportional to return covariance matrix ($$\Sigma_{\pi} = \tau \Sigma$$). Remeber that, for good reasons, the prior mean is extracted from reverse optimization wihout clearly defined uncertainty. This one additional assumption instantly free us from the estimation of $$N^2$$ somewhat vague parameters.

Certainly, it is not the 'correct and accurate' assumption, while it does capture some key points from the full picture and significantly eases the tension on estimation. In practice, higher volatility in security returns generally translates to greater uncertainty in estimating expected returns. 

With the tau factor in play, we arrive at the standard Black-Litterman estimatior in the following form:

$$
\begin{aligned}
\mu^{\star} &= (\tau \Sigma + P^T\Omega^{-1}P)^{-1}((\tau \Sigma)^{-1} \pi + P^T\Omega^{-1}Q) \\
M &= (\tau \Sigma + P^T\Omega^{-1}P)^{-1}
\end{aligned}
$$

Now, the scaling factor $$\tau$$ is all required to determine the unceratainty around prior. Typically, $$\tau$$ is assigned heuristically with a small value close to 0 as "uncertainty in the mean is much smaller than the uncertainty in the return itself" (Black & Litterman (1992)). 

This argument aligns with other methods proposed to set $$\tau$$. Whether to take the uncertainty as sampling error ($$\tau = \dfrac{1}{T}$$) or to infer it from holding in risky asset in reference portfolio ($$\tau = \dfrac{1}{w_{risky}} - 1$$). These approachs generally lead to a $$\tau$$ at the range within 0 and 0.1. [Walters (2013)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1701467) provides a very comprehensive review about the parameter $$\tau$$.


#### The Subjective Omega <a name="subparagraph2"></a>

With $$\tau$$ neatly defining uncertainty around the prior, let's take a more detailed look into the uncertainty around likelihood. Our goal is to embrace a wide array kinds of investment views in the form of likelihood.

It is relatively straightforward to set the uncertainty around investment views generated from statistical models. Forecasting errors, such as the residual variance in a regression, naturally find their place in this context. However, similar to prior mean extracted from reverse optimization, it is indirect to deal with investment views that don't emerge from quantitative methods and lack forecast errors (i.e., relying on subjective judgment). Incorporating such investment views demands extra effort and consideration.

Drawing inspiration from Black and Litterman's approach to model prior uncertainty, we can extend a similar principle to other investment views — linking their uncertainty to return variance. After all, it's the relative level of uncertainty that truly matters. For a subjective investment view k expressed as a linear portfolio ($$p_k$$), we can assume that the uncertainty is scaled based on the portfolio variance. 

Now the question turn to the setting of $$\alpha$$.

$$
\begin{aligned}
\omega_k &= \alpha P_k \Sigma P_k^T
\end{aligned}
$$

It's worth noting that when we apply this structure of $$\tau$$ and $$\alpha$$ to define the uncertainty around the prior and likelihood,  the ratio of $$\dfrac{\tau}{\alpha}$$ alone is sufficient to define our posterior mean ($$\mu^{\star}$$). It is the existence of posterior variance (M) that necessitates the seprate estimation of $$\tau$$ and $$\alpha$$.

$$
\begin{aligned}
\mu^{\star} &= (\tau \Sigma + P^T\Omega^{-1}P)^{-1}((\tau \Sigma)^{-1} \pi + P^T\Omega^{-1}Q) \\
 &= \pi + \tau \Sigma P^T(\alpha P\Sigma P^T + \tau P\Sigma P^T)^{-1} (Q-P\pi) \\
&= \pi + \Sigma P^T (\dfrac{\alpha}{\tau} P\Sigma P^T +  P\Sigma P^T)^{-1} (Q-P\pi)\\
M &= (\tau \Sigma + P^T\Omega^{-1}P)^{-1} \\ 
& = (1 + \tau)\Sigma - \tau^2\Sigma P^T (\tau P \Sigma P^T + \Omega)^{-1}P \Sigma
\end{aligned}
$$

In practice, our primary focus often lies with the posterior mean rather than the posterior variance. This is probably the reason why such a simplification of $$\dfrac{\tau}{\alpha}$$ is untilized in a lot of applications. It is worth noting that, though retains many of the metris of the full scale framework, such approach is no longer a full-fledged Bayesian framework. 

There are also paths to uphold the sanctity of a full-fledged Bayesian framework with both posterior mean and variance provided.

**Idozek Implied Confidence Level**

Following this idea, Idozek(2007) introduced a remarkably intuitive method to infer $$\alpha$$ from the percentage of confidence, as defined by the desired level of active deviation. This approach paints a clear picture of the translation from a subjective confidence level to $$\omega$$.

Depending on our confidence in a particular investment view k, our refined posterior mean range from $$\mu^{\star}_{0}$$ (in case of 0 confidence in the view/ $$\alpha \to \infty$$) to $$\mu^{\star}_{100}$$ (in case of absolute confidence in the view $$\alpha = 0$$). Correspondingly, our final portfolio built on the refined estimation would range from the most conservative portfolio $$x^{\star}_0$$  to the most confident portfolio $$x^{\star}_{100}$$.

$$
\begin{aligned}
\mu^{\star} &= \pi + \Sigma_{\pi}P_k^T(\Omega + P_k\Sigma_{\pi}P_k^T)^{-1} (Q-P_k\pi) \\
\mu^{\star}_{100} &= \pi + \tau\Sigma P_k^T (P_k\tau \Sigma P_k^T)^{-1}(Q - P_k\pi)\\
\mu^{\star}_0 &= \pi \\
x^{\star}_{100} &= (\lambda \Sigma)^{-1} \mu^{\star}_{100}\\
x^{\star}_0 &= (\lambda \Sigma)^{-1} \pi = x_0\\
\end{aligned}
$$

A confidence level (0% ~ 100%) can be defined in terms of the desired position ($$\hat{x}$$) between the two portfolios of $$x^{\star}_0$$ and $$x^{\star}_{100}$$.    

$$
\begin{aligned}
confidence &= \dfrac{\hat{x} - x^{\star}_0}{x^{\star}_{100} - x^{\star}_0} \\
\hat{x} &= x^{\star}_0 + confidence * (x^{\star}_{100} - x^{\star}_0) \\
\end{aligned}
$$

This approach offers an intuitive and consistent means of expressing confidence. On one hand, it provides the flexibility to express confidence in percentage from 0% to 100%. On the other hand, this percentage of confidence is **linearly proportional to our active weight and risk**. As we move our confidence from 0% to 100%, the active weight progresses linearly from $$x^{\star}_0$$ to $$x^{\star}_{100}$$ and so does the active risk.

With the method in place, we simplify the estimation of $$\omega$$ by linking it with a well defined percentange of confidence level. All we need is to back out the $$\omega$$ that leads to this portfolio with desired deviation ($$\hat{x}$$). Walters (2007) provides an analytical solution of this portfolio leading even more simplied results. 

$$
\begin{aligned}
\dfrac{1}{1 + \alpha} &= \dfrac{\hat{x} - x^{\star}_0}{x^{\star}_{100} - x^{\star}_0} = confidence \\
&\downarrow \\
\alpha &= \dfrac{1 - confidence}{confidence} \\ 
\omega_k & = \alpha p_k \Sigma p_k^T \\
\end{aligned}
$$

This approach can be applied systematically to all investment views lacking clearly defined uncertainty, effectively expanding the utility of the Black-Litterman framework to accommodate a wide array of investment perspectives, as long as subjective judgment on confidence level is available.

**Haesen, Hallerbach, Markwat & Molenaar tau/alpha ratio**

There are also ways to undertake the $$\dfrac{\tau}{\alpha}$$ within a full fledged Bayesian framework. For application in asset allocation, Haesen, Hallerbach, Markwat & Molenaar (2017) introduced another modification that simplifies the Black-Litterman framework. They suggest assuming the availability of only absolute views on all securities and directly defining the ratio of $$\dfrac{\tau}{\alpha} $$instead of treating them as separate parameters. This strategic restriction transforms the matrix P into an identity matrix, resulting in significant simplification.

$$
\begin{aligned}
P &= I_N \\
&\downarrow \\
\mu^{\star} &= (1 + \dfrac{\tau}{\alpha})^{-1} (\pi + \dfrac{\tau}{\alpha}Q) \\
M &= (\tau^{-1} + \alpha^{-1}) \Sigma
\end{aligned}
$$

Now within the mix of investment views, what truly matters is the ratio $$\dfrac{\tau}{\alpha}$$. The weights assigned to these views in the refined forecast are **linearly proportional to the ratio**.

$$
\begin{aligned}
\alpha \to 0 (\dfrac{\tau}{\alpha} \to \infty)\\
\mu^{\star} &= Q\\
M &\to 0\\
\alpha = \tau  (\dfrac{\tau}{\alpha} = \dfrac{1}{2})\\
\mu^{\star} &= \dfrac{1}{2} (Q + \pi) \\
M &= \dfrac{1}{2} \tau \Sigma \\
\alpha \to \infty (\dfrac{\tau}{\alpha} \to 0)\\
\mu^{\star} &= \pi \\
M &= \tau \Sigma \\
\end{aligned}
$$


### More on Expected Return Forecast <a name="more"></a>

The techniques discussed above should prepare us reasonably in putting Black-Litterman model in real world application. Wihout doubt, Black-Litterman (or more generally the Bayesian framework underlying) is a powerful and intuitive tool. It enables us to start from the reference portfolio and combine more forecasts by deviating. 

At this point before we call an end on this blog. I feel it deserves a bit more brainstorm on the idea of combining multiple forecasts to refine forecast on expected return and BLack-Litterman's role in this effort at the first places.


#### The Refinement of Multiple Forecasts <a name="subparagraph3"></a>

To me, in the quest for refined expected return forecasts, the idea of combining multiple forecasts presents a tantalizing prospect, rich with potential benefits from various perspectives.

**The Finance Perspective**

The world of financial markets is a labyrinth of complexity, where a myriad of factors exert their influence on expected returns across different time horizons and securities. Yet, due to the inherent challenges of modeling and data limitations, there's no single, unified model or framework capable of a comprehensive review on these factors all at once. Instead, the approach to expected return forecasting often follows the wisdom of "divide and conquer."

Consider, for instance, the long-term horizon spanning decades. Here, fundamental variables like population growth and technological advancements play a pivotal role in forecasting returns on asset classes. Zoom in a bit, and we're focusing on a horizon of a few years, where variables related to the business cycle, such as consumption and production, take center stage in forecasting returns on asset classes, sectors, and more. As you narrow down to shorter horizons, company-specific characteristics become the focal point for equity forecasting.

Likewise, the impact of various factors varies significantly across distinct types of securities. Take, for instance, the realm of equities, where company-specific attributes like value and quality assume paramount significance in the pricing equation. In contrast, within the domain of debt, a different narrative unfolds, with factors such as solvency and leverage assuming a position of preeminence. Meanwhile, in the universe of commodities, it is the ebb and flow of inflation rates that wield substantial influence.

With these specialized models, each calibrated for specific horizons and selected subsets of securities, the rationale for combining forecasts becomes much clearer. By aggregating insights generated from these diverse models, we piece together a more comprehensive view of the vast and intricate financial landscape, enabling us to navigate its complexities with greater clarity and insight.

**The Data Science Perspective**

The notion of combining multiple forecasts intersects with a trailblazing effort in data science aimed at enhancing forecast accuracy. Envision it as akin to the concept of ensemble learning, which ranks among the top strategies in many machine learning competitions. The fundamental premise of ensemble learning is to amalgamate individual models to craft a more robust meta-model.

Drawing a parallel to the Black-Litterman model, think of it as an ensemble model comprising a pool of distinct individual models. Within this ensemble, those models exhibiting relatively greater strength are accorded higher weights. This mirrors the essence of the Black-Litterman approach, where investment views endowed with lower uncertainty wield greater influence over the final refined estimate.

In essence, it's reminiscent of a fusion of bagging and stacking techniques. Heterogeneous models are applied to diverse datasets, and this methodology thrives when applied to a varied array of models—an apt analogy for the diverse landscape of expected return forecasting.



#### More on Black-Litterman <a name="subparagraph4"></a>


Now turn back to Black-Litterman model and the associated Bayesian framework again. At its core, it takes various expected return forecast likely generated from different models and combine them into one refined new estimation. In this process, although named and used differently, the difference between the forecast in terms of prior and the forecast in terms of likelihood is smaller than it seems. They are all taken as forecast on expected returns with uncertainties. 

In my view, what make the pior stand out from all expected forecast are two folds. For one hand, the prior forecast do have opnion on every security within the universe, which build the base line for other partial forecasts. 

a lot of factor impacting expected return. looking into different horizon asset type lead to different key drivers. hard to get an integrated model. divide and conquer is a solution. 




### Reference <a name="ref"></a>

- Walters (2013): The Factor Tau in the Black-Litterman Model
- Walters (2007): The Black-Litterman Model in Details
- Idzorek (2007): A step-by-step guide to the Black-Litterman model
- Haesen, Hallerbach, Markwat & Molenaar (2017): Enhancing Risk Parity By Including Views
